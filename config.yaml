model:
  class: "custom_models.qwen.Transformer"
  pretrained_model_path: "Qwen2.5-3B-Instruct"
  device: "cuda"
  dtype: "bfloat16"

generation_strategy:
  _target_: "generation_strategies.multinomial_sampling.MultinomialSamplingCLM"
  max_gen_len: 1024

task:
  _target_: "tasks.countdown_task.CountdownTask"
  data_path: "Countdown-Tasks-3to4"
  test_size: 128
  tokenizer: "Qwen2.5-3B-Instruct"

optimizer:
  learning_rate: 1.0e-5
  weight_decay: 0.0
  betas: [0.9, 0.999]
  use_memory_efficient_adamw: true

objective:
  _target_: "objectives.grpo.GRPO"
  micro_batch_size: 2
  max_grad_norm: 1.0

training:
  random_seed: 1337
  max_prompt_len: 256
  batch_size: 256
  num_questions_per_batch: 32
  micro_batch_size: 2  # Number of examples per gradient accumulation step
  max_grad_norm: 1.0
  ckpt_dir: "ckpt"
  log_dir: "logs"
  skip_unfinished_episodes: false
  ckpt_save_interval: 100
  eval_interval: 10
